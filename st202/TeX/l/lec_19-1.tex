% ! TeX root = ..\main.tex
\lecture{19}{Lecture 1}{Tue 8 Mar 2022 14:00}{}

\subsection{More on Sufficient Statistics}

\begin{eg}
Let $Y_1,\ldots,Y_n\sim \Normal(\mu,\sigma^2)$. Then
\begin{align*}
    \mathcal L(\mu,\sigma^2;\yy)&=(2\pi\sigma^2)^{-\frac{n}{2}}e^{\frac{-\sum^n_{i=1}(y_i-\mu)}{2\sigma^2}} \\
    &=(2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}}\left(\sum y_i^2-2\mu\sum y_i +\mu^2\right) \\
    &=(2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}}\left(\sum (y_i-\bar y)^2+n(\bar y -\mu) \right) \\
	&=(2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}e^{-\frac{(n-1)s^2}{2\sigma^2}}e^{-\frac{n}{2\sigma^2}(\bar{ y-\mu)^2}}.
\end{align*}
\end{eg}

There are multi dimensional sufficient statistics. Grouping these differently gives different sufficient statistics. Which one is preferable?

Let $Y_1,\ldots,Y_n\sim F_Y(y;\theta).$ Then $\YY$ is a sufficient statistic! 

Suppose that $U=\sum^n_{i=1}Y_i$ is sufficient for $\theta.$ Let $\mathbf V=\left(Y_1,\sum^n_{i=2}Y_i\right)$. Then $\mathbf V$ is sufficient since you can figure out $U$ from $\mathbf V.$ But it doesn't make sense to use $V$, since it is two dimensional. Notice that $U=g(\mathbf V).$ What does this tell us?


Suppose that $V$ is a statistic. If $U$ is a function of $V$ alone (i.e., $U=g(V)$), then
\begin{prop}
\begin{enumerate}[i.]
    \item $U$ is a statistic;
    \item if $U$ is sufficient for $\theta,$ then $V$ is also sufficient;
    \item if $V$ is not sufficient, then $U$ is not sufficient;
    \item if $V$ is sufficient and $g$ is injective, then $U$ is also sufficient.
\end{enumerate}
\end{prop}

\begin{proof}
Exercise 10.1 in the course textbook.
\end{proof}

\begin{definition}
A statistic $U$ is a \textbf{minimal sufficient statistic} if, for any other sufficient statistic $V$, $U$ is a function of $V.$
\end{definition}
A minimal sufficient statistic has the lowest number of dimensions among all sufficient statistics.

\begin{prop}
Let $\YY=(Y_1,\ldots, Y_n)^T$ with mass/density $f_\YY(\yy;\theta).$ If we can find a function $h$ such that:
$$
h(\yy)=h(\xx) \iff \frac{f_\YY(\yy;\theta)}{f_\YY(\xx;\theta)} = k(\yy,\xx)
$$
with $\xx=(x_1,\ldots,x_n)^T$ and where $k$ does not depend on $\theta,$ then $h(\YY)$ is a minimal sufficient statistic for $\theta.$
\end{prop}

\begin{proof}
Omitted.
\end{proof}

\begin{eg}
Let $Y_1,\ldots Y_n\sim \Normal(\mu,\sigma^2)$. Note that
\begin{align*}
    \frac{f_\YY(\yy;\mu,\sigma^2)}{f_\YY(\xx;\mu,\sigma^2)} &= \frac{(2\pi\sigma^2)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}\left(\sum_i y_i^2-2\mu\sum_i y_i +\mu^2\right)}}{(2\pi\sigma^2)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}\left(\sum_i x_i^2-2\mu\sum_ix_i +\mu^2\right)}}\\
    &=\exp\left(-\frac{1}{2\sigma^2}\left[\left(\sum_i y_i^2-\sum_ix_i^2\right)-2\left(\sum_i y_i - \sum_i x_i\right)\mu\right]\right) \\
    &= k(\yy,\xx) \\
    &\iff \left(\sum_iy_i,\sum_i y_i^2\right)=\left(\sum_i x_i,\sum_ix_i^2\right).
\end{align*}
Then $\left(\sum_iY_i,\sum_iY_i^2\right)$ is a minimal sufficient statistic for $(\mu,\sigma^2)^T$. 
\end{eg}

\begin{prop}
If $\YY$ is a random sample, $\UU$ is a statistic, and $\mathbf S$ is a sufficient statistic for $\theta,$ then $\mathbf T=:\EE(\UU \mid \mathbf S)$ is a \textit{statistic,} i.e., its value does not depend on $\theta.$
\end{prop}
\begin{proof}
\begin{align*}
    \EE(\UU\mid \mathbf S=\mathbf s &= \EE(h(\YY) \mid \mathbf S = \mathbf s) \\
    &= \int_{\RR^n}h(\yy)\underbrace{f_{\YY\mid \mathbf S}(\yy\mid \mathbf s)}_{\text{dnd on $\theta$}} \ d\yy,
\end{align*}
i.e., it does not contain $\theta.$
\end{proof}

The next lecture introduces the Rao-Blackwell Theorem.
